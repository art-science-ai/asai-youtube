

Why do AI assistants suck? One big reason is Apple and Google's tight control over the mobile ecosystem.

I'm super excited about new product innovations powered by AI, but I'm worried about this innovation being stifled by Apple and Google's tight control over the mobile ecosystem! ðŸ™ƒ Mark Zuckerberg pointed this out in his recent interview with Dwarkesh Patel: "One thing that I think generally sucks about the mobile ecosystem is that you have these two gatekeeper companies Apple and Google that can tell you what you're allowed to build ... there's a bunch of times when we've launched or wanted to launch features and then Apple's just like NOPE you're not launching that ... that sucks ... the question is are we kind of set up for a world like that with AI?" (Short video: [https://youtube.com/shorts/tAzfozcpu_o](https://youtube.com/shorts/tAzfozcpu_o) | full interview: [https://youtu.be/bc6uFV9CJGg](https://youtu.be/bc6uFV9CJGg))

AI assistants have the potential to give consumers superpowers in the digital world. For most of us today, our smartphones are our primary windows into the digital world, but these windows (in this case iOS and Android) are locked with safety grills that are obstructing the view, and the keys are tightly held onto by Apple and Google. They use their keys to open the grills for their own AI assistants (e.g. Siri and Google Assistant), but not for anyone else. For example, you can interact with Siri hands-free without taking your iPhone out of your pocket by just saying "Hey Siri", and you can use your voice to engage with other people, businesses, and apps in the digital world (e.g. sending messages, controlling your smart home, etc.), but Apple won't let users interact with any other AI assistant in this manner. Moreover, Siri is extremely underpowered in terms of AI capabilities, so many innovators are eager to get improved AI assistants in the hands of consumers. So what choice do these innovators have?

One option is for AI assistants to live with the obstructed view (e.g. ChatGPT in-app voice chat, Meta AI in Whatsapp and Instagram, etc.). The obstructed view means that you can't engage with the digital world as easily, so you need to pull your phone out of your pocket, open the ChatGPT app, and click the voice button for each and every interaction. The obstructed view also means that you can't meaningfully engage with the digital world, so you can have a conversation with ChatGPT, but you can't use it to interact with other people, businesses, and apps. Similarly, you can use Meta AI in Whatsapp and Instagram, but it can never do anything outside Whatsapp or Instagram like writing emails.

A second option is for AI assistants to build their own windows on the back side of the building (e.g. Meta Smart Glasses, Rabbit R1, Limitless Pendant, Humane AI Pin, etc.). If you build your own window, your view is no longer obstructed by Apple or Google's ecosystem locks and you can do things like hands-free voice interactions, but the problem is that that there's nothing interesting to see out of the windows on the back side of the building! All of the things you want to see and do in the digital world, i.e., all of the people and businesses you want to interact with, are in front of the smartphone windows (i.e. mobile apps). So my Meta Smart glasses can take photos and use Meta AI to ask questions about the photo, but I can't use the AI to interact with my text messages or notes. Another problem is that the hardware for these new windows (devices) is a significant expense which limits consumer adoption.

Is there a third option? I'm not sure. Are we at a tipping point where people and businesses will start moving all around the building to make the view better for all windows? I'm not sure. What do you think?