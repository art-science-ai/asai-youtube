---
source-title: RAG From Scratch: Part 4 (Generation)
source-url: https://youtube.com/watch?v=Vw52xyyFsB8

source-mediatype: video
source-platform: YouTube

source-youtube-video-id: Vw52xyyFsB8
source-youtube-channel-id: UCC-lyoTfSrcJzA1ab3APAgw
source-youtube-channel-name: LangChain
---

## Summary (AI)
TLDR:
- The video discusses the process of generating answers in a retrieval-augmented generation (RAG) model.
- The key steps in the workflow include splitting and embedding documents, searching for relevant documents using KNN, creating a prompt template, and using a language model (LLM) to produce answers.
- The speaker demonstrates coding examples to show how to build a retriever, define a prompt template, connect it to an LLM in a chain, and automate the retrieval process using a RAG chain.
- The video concludes by mentioning upcoming videos that will delve into more complex themes in the RAG model pipeline.

Key Points:
- RAG model focuses on generating answers based on retrieved documents relevant to a question.
- Workflow involves splitting and embedding documents, storing in a vector store for search, creating a prompt template, and using an LLM to produce answers.
- Demonstrated coding examples include building a retriever, defining a prompt template, connecting to an LLM in a chain, and automating the retrieval process using a RAG chain.
- Mention of upcoming videos exploring more detailed themes in the RAG model pipeline.

## Video description
Error retrieving description for video Vw52xyyFsB8

## Video transcript
hey this is Lance from Lang chain this is the fourth uh short video in our rag from scratch series that's going to be focused on generation now in the past few videos we walked through the general flow uh for kind of basic rag starting with indexing Fall by retrieval then generation of an answer based upon the documents that we retrieved that are relevant to our question this is kind of the the very basic flow now an important consideration in generation is really what's happening is we're taking the documents you retrieve and we're stuffing them into the llm context window so if we kind of walk back through the process we take documents we split them for convenience or embedding we then embed each split and we store that in a vector store as this kind of easily searchable numerical representation or vector and we take a question embed it to produce a similar kind of numerical representation we can then search for example using something like KNN in this kind of high dimensional space for documents that are similar to our question based on their proximity or location in this space in this case you can see 3D is a toy kind of toy example now we've recovered relevant splits to our question we pack those into the context window and we produce our answer now this introduces the notion of a prompt so the prompt is kind of a you can think of a placeholder that has for example you know in our case keys so those keys can be like context and question so they basically are like buckets that we're going to take those retrieve documents and Slot them in we're going to take our question and also slot it in if you kind of walk through this flow you can kind of see that we can build like a dictionary from our retrieve documents and from our question and then we can basically populate our prompt template with the values from the dict and then it becomes a prompt value which can be passed to an llm like a chat model resulting in chat messages which we then parse into a string and get our answer so that's like the basic workflow that we're going to see and let's just walk through that in code very quickly to kind of give you like a Hands-On intuition so we had our notebook we walked through previously install a few packages I'm setting a few like Smith environment variables we'll see it's it's nice for uh kind of observing and debugging our traces um previously we did this quick start we're going to skip that over um and what I will do is I'm going to build our retriever so again I'm going to take documents and load them uh and then I'm going to split them here we've kind of done this previously so I'll go through this kind of quickly and then we're going to embed them and store them in our index so now we have this retriever object here now I'm going to jump down here now here's where it's kind of fun this is the generation bit and you can see here I'm defining something new this is a prompt template and my prompt template is something really simple it's just going to say answer the following question based on this context it's going to have this context variable and a question so now I'm building my prompt so great now I have this prompt let's define an llm I'll choose 35 now this introduces the notion of a chain so in Lang chain we have an expression language called Cel Lang expression language which lets you really easily compose things like prompts LMS parsers retrievers and other things but the very simple kind of you know example here is just let's just take our prompt which you defined right here and connect it to an LM which you defined right here into this chain so there's our chain now all we're doing is we're invoking that chain so every L expression language chain has a few common methods like invoke bat stream in this case we invoke it with a dict so context and question that maps to the expected Keys here in our template and so if we run invoke what we see is it's just going to execute that chain and we get our answer now if we zoom over to Langs Smith we should see that it's been populated so yeah we see a very simple runnable sequence here was our document um and here's our output and here is our prompt answer the following question based on the context here's the document we passed in here's the question and then we get our answer so that's pretty nice um now there's a lot of other options for rag prompts I'll pull one in from our prompt tuub this one's like kind of a popular prompt so it just like has a little bit more detail but you know it's the main the main intuition is the same um you're passing in documents you're asking to reason about the documents given a question produce an answer and now here I'm going to find a rag chain which will automatically do the retrieval for us and all I have to do is specify here's my retriever which we defined before here's our question we which we invoke with the question gets passed through to the key question in our dict and it automatically will trigger the retriever which will return documents which get passed into our context so it's exactly what we did up here except before we did this manually and now um this is all kind of automated for us we pass that dick which is autop populated into our prompt llm out to parser now to invoke it and that should all just run and great we get an answer and we can look at the trace and we can see everything that happened so we can see our retriever was run these documents were retrieved they get passed into our LM and we get our final answer so this kind of the end of our overview um where we talked about I'll go back to the slides here quickly we talked about indexing retrieval and now generation and followup short videos we'll kind of dig into some of the more com complex or detailed themes that address some limitations that can arise in this very simple pipeline thanks
