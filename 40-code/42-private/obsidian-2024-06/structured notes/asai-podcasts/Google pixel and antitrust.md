---
aliases: 's2-e9 google pixel'
related-notes: 
tags: 

fileClass: class-asai-episode
templater-template: 

asai-episode-id: 
asai-episode-title: 
asai-publish-date: 

asai-episode-link-youtube: https://youtu.be/cAzqD42yCHY
asai-episode-link-spotify: https://podcasters.spotify.com/pod/show/nikhil-maddirala6/episodes/S2-E9-Google-AI-announcements-and-legal-challenges-e2nm09a
asai-episode-link-apple: 
asai-episode-link-substack: https://artscienceai.substack.com/p/s2-e9-google-pixel-ai-and-legal-challenges
---


# Pre-episode Notes

# Editing
Full episode
- [x] Rename studio recording to episode title > generate full episode edit
- [x] Set pace: 1.5s + pause (DO NOT remove filler)
- [x] Intro + outro clips (incl. cutting filler)
- [x] Orientation: Nikhil on left / top
- [x] Remove captions
- [x] Download subtitles
- [x] Export > Normalize audio + remove background noise

Shorts
- [ ] Orientation: Nikhil on left / top
- [ ] Captions > Background, Poppins Bold, medium, boxed, Red highlight > in middle
- [ ] Export > Normalize audio + remove background noise

# Episode Transcript
Nikhil Maddirala (00:10)
Hey, welcome to the Art and Science of AI, a podcast about the science of how AI works and the art of using AI to reimagine your life, business, and society. I'm Nikhil Madhiraala. I'm one of your co -hosts. I'm an AI product manager, and I love helping people learn about AI.

Piyush (00:30)
I'm Piyush, your other co -host. I'm an AI sales executive, and I love learning about AI and talking about AI. Nikhil, what's going on in the world of AI?

Nikhil Maddirala (00:39)
Yeah, so interestingly, a lot of stuff's been going on with the company you work at, Google.

In the last episode a couple of weeks ago where we talked about AI news, we talked a lot about Meta, Apple, and OpenAI. There were lot of announcements last month around that. But Google has been in the news in a big way for the past week or so. A couple of things have been going on that I want to talk about. One is their recent event announcing the Pixel phones and the associated AI features. And second is there's been some legal

case against Google in the US an antitrust case that I think is worth talking about. Yeah.

Piyush (01:21)
Yeah, folks wanna break up Google, I don't feel so good about it being an employee of Google. it is what it is, I guess.

Nikhil Maddirala (01:28)
That's true. I mean, I don't think it should impact employees and shareholders. I was actually reading that the last major antitrust breakup like this, was of Rockefeller's companies, like the oil companies that he had back in the 19th, 20th centuries, after the breakup, the combined valuation of the broken up companies was way higher than the single company. And so anyway.

Piyush (01:56)
That's a good point. A good argument can be made that YouTube outside of Google, not that the way that will be broken up will be Google separate, YouTube separate, but YouTube outside of Google could easily be valued at a trillion dollars, maybe at least like a half a trillion dollars. And I don't know how much of that is baked into Google's valuation. So wow, in like a minute now you made me feel better about the breakup. Anyways, let's talk about the event. Yeah, yeah.

Nikhil Maddirala (02:12)
You

Let's start with the pixel. Yeah, the pixel event. So Google had their pixel event, is the equivalent of Apple's iPhone event. And I think on everyone's minds is Apple intelligence, which came out a couple of months ago. So when Google talks about their new

phones and features, everyone's wondering about what's going on with AI. So in the event, they announced, obviously, their hardware also. they announced updates to Google Pixel phone, the Pixel watch, and the Pixel buds, which are the earbuds. But in addition to that, the star of the show was kind of their AI announcements, which were a through line through all of these different devices. And yeah, the AI kind of works across all the devices.

And for the purposes of our podcast, think let's skip over the hardware stuff that that's not what we're focused on. And let's talk about what AI stuff they announced.

Piyush (03:22)
Yeah, sounds interesting. Let's talk about it. I mean, you'd think that being a Google employee, I would know exactly what's going on. But there is like, it's a giant company and there's so many announcements that are happening. There's so many events. Like I have enough news and events to keep up within the advertising ecosystem. I don't always get a chance. So I'm actually, it's weird, but I'm looking forward to hearing what my company announced recently on AI from you.

Nikhil Maddirala (03:32)
Yeah.

True, this happens to me also, especially when Meta announces VR and Metaverse hardware, often I just find out about it through the news. I mean, I could be following this internally, but yeah, we both work at companies that are just so big that there's not enough time in the day for me to follow everything that's going on everywhere.

Piyush (04:10)
Yeah, for sure.

Nikhil Maddirala (04:12)
So, right. So first we said that they launched their hardware and then they're the AI features. So the, I think the most interesting AI feature they announced is this thing called Gemini Live. So Gemini is the Google AI model, or it's the brand of their various AI models. And

This new thing they announced called Gemini Live is basically a real time interaction that you can have with Gemini using voice and it simultaneously has access to things like your camera, your screen and various information from your apps. So it's kind of like what Shad GPT OpenAI announced a couple of months ago about voice mode.

where remember they were they got into this controversy with Scarlett Johansson yeah about that so that was the real -time voice capabilities that they announced which actually has not yet been rolled out they've claimed that they've started rolling it out in some kind of testing but i haven't seen that and i haven't heard of anyone who's seen that yet so we're still waiting for that

Piyush (05:00)
Yeah.

It actually reminds me more of what Apple announced, because you said three very important things, right? It will have access to your camera, the app. So I think you're right. The voice mode is very similar to what OpenAI announced. But as you were saying, those things in my mind was going to our discussion on the Apple intelligence episode, right? It sounds like it's one of those inbuilt within the operating system type AI things, which can seamlessly bring information from apps, from hardware, obviously, like camera, mic, or whatnot.

Nikhil Maddirala (05:37)
Yup.

Piyush (05:49)
like be your.

Nikhil Maddirala (05:50)
Yes, absolutely. I think they are competing with Apple intelligence here, but the one big thing that's missing in what they announced is integration with third party apps. So unfortunately they haven't announced anything around that yet. There are rumors that they will announce something, but so far everything they announced. So remember when we talked about Apple, we talked about different categories of announcements they made. One was around.

better AI in Apple's apps, like in mail, photos, messages, and those kind of apps. And then there's AI in Siri. And the AI in Siri can work with Apple's apps, but it can also work with third -party apps. And they released developer frameworks for developers to...

to integrate with that. In this case, Google has announced making their app smarter and they've announced that the assistant is going to get smarter with Gemini. But from what we've seen, it's limited to only Google's apps. So it can give you answers if you have questions about things where the answers are in your Gmail or in Google Maps or YouTube or somewhere. It can give you those answers. But if your questions about something in WhatsApp

or anything outside of Google, like it's not going to be able to work with that. So I think we should watch out and see what they are going to announce for that because I do think they have to offer something there. Otherwise it's just like not going to be as competitive as Apple intelligence.

Piyush (07:25)
Yeah, but as far as starting off with proprietary apps, good thing is that Google's ecosystem has such an insane list of proprietary apps with such high usage that even starting off with just Google ecosystem is not a bad place to start with. think now it's hard to keep count, over 10 property or 15 Google properties that have over half a billion users monthly and some six Google properties that have more than two billion users monthly, like for example, Maps.

Nikhil Maddirala (07:48)
Wow.

Piyush (07:55)
Gmail, Chrome, Play Store, all of these are the big properties. But then there's a long list of things, like Google Photos, whatnot. So even within this ecosystem of 15 properties, I think you can make for a really good integrated AI, right? Like, hey, pull my photos from this thing, or connect some email with the calendar. And am I free? I think there was one demo that did trickle down to my consciousness, and it was a failed demo. Did you get a chance to watch it?

Nikhil Maddirala (07:58)
Yeah.

yeah, yeah. Yeah, so I actually watched it and they had to try doing the demo like two or three times before it succeeded. It was a demo of this Gemini thing. And I think that's because previously they did a demo that was not live and they got a lot of criticism for that saying that, they were deceiving the audience into showcasing some capabilities that were not real. So.

Piyush (08:29)
Yeah.

Right, right.

Nikhil Maddirala (08:48)
I think it's fine. Like, I mean, people understand that things fail.

Piyush (08:50)
Yeah, no, I mean, I wasn't so much talking about like the failed aspect of the demo, but like what they were trying to achieve in that demo was something like, hey, like, see if I'm free on this evening when the con is.

Nikhil Maddirala (09:00)
Yes, exactly. So in the demo, what they were doing was they were looking at a

Concert schedule. I forgot if they took a photo of the concert schedule or if they pulled it up on a website one of those things in either case The Gemini assistant can answer that question and interact with it whether it's on your screen or whether you took a photo of it So some concert and then the person asks to check their calendar to see if they're free on the date of the concert and It checks the calendar and says hey you're free and then they asked to create a reminder to buy the concert

Piyush (09:07)
Right.

Right.

Nikhil Maddirala (09:35)
tickets and it integrates with the Google Calendar to check whether they're free and then Google has a reminders or tasks app where it adds this item to remind you to go and buy tickets. Yeah.

Piyush (09:36)
Yeah.

very cool. It's actually something like an assistant would do for a person who can afford an assistant. Like, hey, this artist that my daughter likes is check if they're coming to our city and if I'm free that day. And then you just assume that the assistant will then search for the dates and then find out, look at your calendar. It's kind of cool. And I imagine that this is what originally the name assistant, that's why the name assistant would add it to Google Assistant, but it's kind of.

Nikhil Maddirala (09:55)
Yeah.

Piyush (10:17)
surprising that we're going with the Gemini branding here.

Nikhil Maddirala (10:20)
Yeah, I mean, so first of all, absolutely. I think these capabilities are really cool. On the branding, yeah, that I can't answer. I think Gemini is the AI brand and they could have chosen to just say Google Assistant now powered by AI, but I think it sounds cooler if they say Gemini. But essentially you can replace your Google Assistant with Gemini now. I don't know if that is...

Piyush (10:41)
Gemini.

I'm not allowed to share this, but I've been dogfooding it. So for those of you who don't know what dogfooding is, it's basically employees internally at Google can get access to early features. I have been dogfooding Gemini as my Google Assistant. And it's pretty neat. With Google Assistant, would barely use it with just what's the weather like, set an alarm, stuff like that, just stuff that I could do manually even. But I choose the Assistant because it's just a voice command.

Nikhil Maddirala (10:51)
Okay

Piyush (11:16)
Gemini as the assistant, you can ask it more complex questions. Almost like you're prompting your AI. But that's different than Gemini Live, though. I think Gemini Live is just a more natural.

Nikhil Maddirala (11:21)
Right.

Well, before we get into Gemini Live, it's interesting that you mentioned this, that you were able to do only basic things earlier, like setting an alarm or checking the weather, and now you can do more advanced things. But one of the common complaints I've been seeing from users is that now you can do more advanced things, but you can't do the basic things.

Piyush (11:45)
But you can do the basic things. Yeah. So the annoying is I can't turn on and off my like, have to do that with my finger.

Nikhil Maddirala (11:51)
And you can't set a timer, like basic functionality, like, set a timer for five minutes. You can't do it.

Piyush (11:54)
Yeah. Yeah. Yeah. Yeah. That's the whole idea of a dog fooding is like you, I mean, I do give a lot of feedback as well. Like, and one of the feedback has been like, yes, a lot smarter, but not able to do some things that a dumb version could do.

Nikhil Maddirala (12:09)
Yeah.

So apparently this is a really hard problem to solve, which is how do you do both the simple things and the hard things? Because for the simple things, you don't want to route it to this LLM. You just want a simple deterministic program to set the timer for you. But then for some complex queries, you do want to route it to the LLM. Just solving this challenge is incredibly complex. So one of my best friends who works on Amazon, Alexa,

Piyush (12:24)
Right.

Nikhil Maddirala (12:41)
Alexa was telling me this because he's trying to solve this problem for Alexa. I've been trying to convince him to come on this podcast for a while but I have been unsuccessful. If I can get him on, we should talk about it. But yeah, he's been actually trying to solve this exact challenge for Alexa which is how

Piyush (12:50)
That would be an interesting thing to talk about, yeah.

Why, I mean, again, just not knowing much about this, why couldn't you use the same kind of architecture that you described that Apple may be using is like there's something that uses the judgment on where to send it, right? You said something to that effect when we're talking about Apple intelligence, that it has some kind of an architecture where first a judgment is applied to what needs to be used. So like, why couldn't there be something that...

Nikhil Maddirala (13:15)
Mm -hmm. Mm -hmm.

Yeah, so first you have an orchestrator model or agent that determines where to route your request and whether it needs to be routed. I think something like that is the answer.

Piyush (13:27)
Yeah.

Nikhil Maddirala (13:36)
But the main challenge is that it's, so if that router or orchestrator agent is an LLM based agent, the thing is it is very difficult to get consistent outputs from any type of LLM application right now. So that's one of the main challenges. If I want consistent structured output is very challenging. It requires a lot of finesse in crafting the prompt, in using things, advanced tool capabilities of LLM.

Piyush (13:52)
Right.

Nikhil Maddirala (14:06)
like function calling and that's one of the hardest things that like we have not solved yet. So LLMs are great for things like chatting, having natural language conversations, but when you want them to behave like a computer program and do something deterministic or quasi deterministic like providing structured outputs and routing, that's where...

It's a hard problem to solve and we haven't figured it out yet. Yeah, if we can get Jay on here, like he can tell us more about it, but until then, it's the same problem Google is facing. And so they've released this Gemini Live does a bunch of cool things. So yeah, the live part of this is that it's real time.

Piyush (14:33)
Interesting.

Nikhil Maddirala (14:51)
It's very low latency. You can have a conversation with it. You can interrupt it right in the moment You can show new context to it So suppose you have your camera open and you're chatting with it You can move the camera to something else and say hey tell me what this is now or if you are Chatting with it based on what's your screen You can like scroll your screen to the next page or to a different website and keep asking it questions Like hey, what's on this website then what's on the next page and things like that? So it's like

Imagine your assistant has access to your eyes and ears somehow. So it can see what you're seeing, can hear what you're hearing, and it has access to information in your app. So like the calendar example that we saw. And one other thing they announced was that it's also gonna be able to capture history and have memory through screenshots. So if you take screenshots of things you want to remember,

Piyush (15:25)
Yeah.

Nikhil Maddirala (15:48)
then you can go back and ask it just when you're chatting with it. You can say like, hey, tell me about this thing I saw yesterday. This is kind of like Microsoft Recall where Microsoft was automatically taking screenshots of your screen periodically so that if you just want to remember something that happened yesterday, you're like, hey, I saw an interesting product on Amazon, but I totally forgot what it was. Tell me about it. It'll tell you that. I think right now the way Google has implemented it, you have to manually take

screenshots maybe in future they'll have some automated way of doing that as well so yeah those were the updates to Gemini and it's pretty cool like I'm excited to to try it out or like maybe when you I I'm an Apple user but you use Android and pixel so I don't know when you get the next phone what do you do for an upgrade of your phone

Piyush (16:44)
I am actually, I have a 7 Pro and this one was 9 or 9 Pro. So yeah, I think I've overdue an update. I'm actually looking forward to it myself.

Nikhil Maddirala (16:48)
Okay, cool. Well, if...

Alright, yeah, well we can talk more about your experience with it when you get it. So that's most of the announcements on Gemini. They also announced a bunch of cool features using Gemini in the apps. Like one is in the photos app. announced some cool things. Magic Editor is a feature where you can modify images of photos that you took. So you can say you took a photo of me like just like this. You could get it to modify the background and say hey put me in

like a different room you could get it to do things like change my hoodie color from red to yellow, remove my headphones.

Piyush (17:32)
Do you think all this is also happening through LLMs or is this Gemini now seems to be the everything branding for AI?

Nikhil Maddirala (17:41)
These are image generation models. So similar to Mid Journey and Dolly, which we were using earlier, but like now they're integrated into the chat bot. So for example, if you're using chat GPT, it can do image generation for you. It's calling an image generation model on the backend. But what the ultimate goal that all these models are trying to get to is what's called a multimodal model.

Multimodal means it supports multiple modalities. modality here is like text, image, audio, video. So these are the four primary modalities. There are primarily three. You can say video is just a combination of image and audio, but it's a different modality. all of these ultimately are going to converge to multimodal models where it's not just text in, text out. Like the models we use today,

that we're familiar with like chat GPT, Gemini, those are typically called text to text models where the input is text and the output is text. And often what they're doing is in the backend, they're calling some other image generation model. So that's called a text to image model. And then they're like image to image models as well. But

Ultimately, the holy grail that people want to get to is multimodal to multimodal models, where you can input anything between text, image, audio, video, and you can get the output in any of those formats that you want.

Piyush (19:19)
Yeah, OK. Yeah, that's going to be very, very interesting, the world to live in, where you have this insanely capable multimodal model, basically.

Nikhil Maddirala (19:28)
And it's really complex to do that because remember in the first season, we talked about how these text models work. They create an abstract representation of the text in like a vector embedding space. So they're creating representations of your words and sentences. So now what's happening is that this representation space captures not only text, but it also captures images and videos. And they need to make a common representation space.

that just understands meaning behind anything whether it's a text or image or video and that actually is a really

Piyush (20:06)
Yeah, basically what humans do, right? Like I'm guessing, I don't know if that's true or not, but that's the goal. I think increasingly it's very obvious to me that the goal is to bring human -like capabilities, right? Like we're multimodal beings. We can hear stuff, we can read stuff, and our output can be in like voice or written.

Nikhil Maddirala (20:19)
Yeah.

We can't directly output like images and video unfortunately, but yeah, I think you can in your mind maybe you can

Piyush (20:31)
Well, we can. Yeah, I mean, the way we output them would be through a sketch. But you know what I mean? That would be our output modality of an image. yeah, think that's the maybe. So perhaps some advanced models of AI would also have touch as a modality or an understanding of, I don't know, what is it called technically? Haptic? I know some of the Mera.

Nikhil Maddirala (20:38)
Yeah.

Yeah.

Hmm.

Hmm.

Piyush (20:58)
Like, I don't know if there's haptic feedback in the most recent Oculus. What is it called now? What's the branding?

Nikhil Maddirala (21:06)
it's been there from the beginning. One of my favorite games that I play on my Oculus is VR table tennis. And it is the most realistic experience I have. the controller is like the table tennis racket. When you hit the ball, it gives you haptic feedback. It buzzes and you know that you hit the ball. And if you hit it at different...

levels of intensity if you hit it harder it gives you a harder feedback if you hit it with spin it gives you a different kind of feedback so that's already there I'm not sure how that gets integrated into AI that's an interesting topic though and for the metaverse that that would be really cool and there's also like smell for example like smell and touch yeah these are the two things that are remaining

Piyush (21:42)
Yeah.

Yeah, because I mean, I imagine there will be some sensors in your phone eventually where you'll be like, what is it that I smell? Hey, Gemini, what do I smell? What's that weird smell? Or potentially, it could smell some dangerous stuff. So there are some good implementations of, I smell carbon monoxide. I don't know if carbon monoxide has a smell. Maybe it's not detectable to humans, but it will be to the AI. So yeah. Yeah. Yeah, yeah.

Nikhil Maddirala (22:01)
Yeah.

Yeah, or a gas leak or something like that.

Absolutely and for touch I think typically one simple way you could do it is you could wear like a glove that has sensors on it and you could touch something and It would tell you I mean that would just be additional information say I'm looking at a tree and like hey I want to learn more about this tree and like if you can touch it it you know understands the texture the contours of the Tree and it's just more information. So yeah, I don't know if

Piyush (22:41)
Yeah.

Right. So the more modalities you add, basically you get a more complete picture of the world that we inhabit. that's hopefully the goal. If you said that the goal is to build an abstract representation with the text to text model, was just in text, right? But our abstract representation of the world is not in one modality. It's in the combination of different modalities, like what we discussed.

Nikhil Maddirala (22:50)
Mm -hmm. Yeah.

Yeah.

Right.

Piyush (23:13)
Perhaps the goal eventually would be to build a system or a model that has an abstract representation of the world that we inhabit. Well, actually, these are the modalities that are applicable to us, right? The five modalities, like touch, smell. But maybe there are other modalities that are outside of the human space, and you could add them to the model even. I can't even imagine what that model would be like.

Nikhil Maddirala (23:21)
Mm -hmm.

Yeah.

Yeah and even within the modalities that we have sensory consciousness of we only have a limited range for example hearing there's a limited range of frequencies we can hear

Piyush (23:45)
Right.

Nikhil Maddirala (23:49)
whereas dogs have a different range of frequencies. That's why you can have something like a dog whistle, which a human cannot hear, but like a dog can. So yeah, I think ultimately maybe AI will have more sensory consciousness than even humans can. I don't know. That would be interesting.

Piyush (24:07)
Yeah, we digress, but just to close the point, a lot of times, people, when they talk about AGI and how when there will be an AGI and it will be on this takeoff of intelligence explosion, it will have an IQ that is different to humans in the same way how human IQ is different from ants. And it's always hard to put that into perspective in my head.

Nikhil Maddirala (24:34)
Yeah.

Piyush (24:36)
But this conversation, I think, helps me put that into perspective. It's like, look, we have five modalities. And to your point, even there we have a limited bandwidth. Our consciousness is only limited. I think some philosopher did some analogy. And he said that we can only process a certain amount of bits, whereas there's gigabits or terabits of data that we could potentially process. Our consciousness could be expanded. So not only can it expand. And I guess the analogy would be the context window, which is just increasing day by day. That would be the short term.

Nikhil Maddirala (24:54)
Hmm.

Piyush (25:06)
conscious that we have. Not only would that be expanded, which we have obviously an example of expanding context windows, but even the modalities would be more than the five that humans have access to. So that maybe gives us some perspective into why this entity, AGI, will be so different to us in its intelligence, how we are to ants. Maybe even worse, or when I say worse, maybe even more difference in orders of magnitude.

Nikhil Maddirala (25:30)
Yeah, I can't even picture what that would be like, but I'm curious and that's exciting. Let's wrap up the Google...

Piyush (25:33)
Okay.

Right, okay. Well, these are the seeds, or these are the early developments that will eventually lead into that. Which is why I feel like this space is so exciting, because we're witnessing something remarkable.

Nikhil Maddirala (25:50)
Yeah.

And so far, all this stuff that we're seeing is focused only on the digital world because AI interacts primarily with the digital world. So that's why we don't see much about like touch and smell is mostly just like image text videos, because that's what we interact with. But I think as metaverse advances with things like augmented reality, mixed reality, virtual reality, that's also going to spur innovations in these other modalities that you're talking about as we're breaking down the barriers between

the digital world and the physical world. So I think that's a next boundary. And yeah, I'm curious to see what more mixed reality devices and hardware are coming out.

So let's wrap up this Google event. I think that we covered most of it. Couple of other features. So we talked about photos having magic editor. They now have this feature called like add me, which is if you want to take a photo with like a group of people and you don't want to take a selfie and you don't have anyone else to take it with you, they let you take a photo without you in it. And then you can go and stand in the empty spot and ask someone else to take a photo with you in it. And then it will stitch both of those together.

to add you in into a group photo. It's kind of gimmicky but I think it's an interesting feature. And the last thing was they have call notes now so you can set it up such that all your phone calls will automatically be transcribed and you will have summaries of those saved onto your phone.

Piyush (27:26)
That's a good one. As long as it's designed with privacy in mind, I think that could be a good one to have.

Nikhil Maddirala (27:28)
Yeah.

Yeah, privacy and consent, I think. So they talked about how they're going to make sure that it's announced and disclosed to everyone on the call, that it's being recorded. And I think it's also just being saved on your device and not going to any cloud. So it is private.

Piyush (27:36)
Right.

Thank

Nikhil Maddirala (27:49)
Let's jump to the other topic, which is... So the other reason Google has been in the news this last week is because a US judge ruled that Google is a monopolist in an antitrust case. So apparently Google has a monopoly over the search advertising market. I'm not sure exactly what the definition of the market they used is, but they declared that Google is a monopoly. And once you're a monopolist, there's certain kind of behavior

Piyush (27:55)
Thank

Nikhil Maddirala (28:19)
that you cannot engage in. And one example of that kind of behavior is making exclusive deals and partnerships. So in this case, the most relevant thing here, which we've discussed in this podcast in the past, is that Google has a deal with Apple for being the default search provider on Apple's

iPhone devices default browser, is Safari. So by default, whenever anyone gets an iPhone and they open Safari and they search for something, that's a Google search. And for that privilege, Google pays Apple around 30 % of all the revenue that they get from the searches. And that translates today to about $20 billion per year, which is huge for Apple.

Piyush (29:14)
Insane.

Nikhil Maddirala (29:15)
Yeah, and for Apple it's pure profit. It has zero cost associated with it.

Piyush (29:20)
Yeah. In fact, they outsource the cost of having a great search engines on iPhone to Google, like all the processing needed, whatever processing is needed is done by Google and you paid for it. That's amazing.

Nikhil Maddirala (29:32)
Yeah. Yeah. So this is the power of controlling distribution. Like Apple controls the distribution. They have the consumers on there. And once you control the distribution, then you can make deals with suppliers and just take a cut out of whatever money suppliers are making. And that's exactly what they're doing. But in this case, a judge ruled that

Piyush (29:39)
Right.

Nikhil Maddirala (29:57)
it was illegal. So that has to like get killed. I mean, of course they will appeal it and there will be a whole appeals process, but let's assume for the moment that this ruling stands.

Let's think about what happens here. This is actually pretty interesting. And it has interesting implications for AI because as we talked about in the Apple intelligence episode, what Apple is currently doing with Google, with search, is the same thing that they're trying to do with AI, where they're saying that, we won't provide...

all the AI capabilities ourselves will let other people like OpenAI and other providers provide it. And we'll just take, I mean, they didn't officially announce anything about like how much money they're taking, but I think most people assume that it's a similar arrangement that someone can bid to be the default like AI provider for Apple and they'll pay them a cut. And.

Piyush (30:54)
That's a very interesting perspective. I didn't think about it like that. But now that you mention it, that's a very, very interesting perspective. And I kind of agree with you. That's very smart, actually. just to repeat, so the arrangement that exists between Apple and Google for Google search, or for search, you're saying you can use the same analogy, or almost exactly the same thing is happening for AI now, where OpenAI could be the Google equivalent of AI.

Nikhil Maddirala (31:09)
Mm -hmm

Yes, yeah. So when they announced it in the Apple intelligence.

Piyush (31:22)
Wow, okay, interesting.

Why not, right? Sorry to interrupt you, but why not? This is such a great arrangement, like we just said. $20 billion of pure profit. This is amazing. So why not use it for AI even?

Nikhil Maddirala (31:33)
Yeah.

So the risk of it is that you don't as a company, you don't want to outsource what is like really important to your customer's experience. And if it turns out that this AI thing is really important and they outsource that, that that could be a risk for Apple in future. Because ultimately, what's maybe it'll get to a point where people don't care about.

the device anymore, they're like, I just want the best AI and they're like, well now whichever device offers the AI assistant that I want, I'll go use that device. So that's the risk for Apple in future that they could become obsolete. I think actually in, I don't know much about China, but from what I've heard there, they have these like super apps, like WeChat, for example, which is almost everything. And

Piyush (32:27)
Yeah, yeah. Which is what Elon Musk is trying to do with X. Like that's his vision with X. He wants to make like the first super app in the Western world.

Nikhil Maddirala (32:37)
And so one thing that happened the risk of allowing super apps is that your Platform and device kind of becomes irrelevant if you're just like hey everything I need to do is on WeChat then I don't care what device or what platform I do that on so that's the risk But before getting to AI, let's actually think about what happens here now that this deal is dead There are like a bunch of interesting possibilities and many questions that are raised

So one is like, yeah.

Piyush (33:08)
Actually, I have a POV on this. So one of the reasons why you have such strong anti -competition laws is to prevent users or consumers from exploitation, right? And the idea was traditionally that, let's say, if a company that makes, I don't know, baby formula had a monopoly, they could start price gouging. And it's not fair to consumers to prevent.

Nikhil Maddirala (33:23)
Mm -hmm.

Piyush (33:37)
monopolistic behavior or like companies from attributing monopolistic behavior. However, in the case of digital or like this doesn't just apply to Google, right? Like a lot of people say Apple is monopolistic or Amazon.

Nikhil Maddirala (33:48)
So, by the way, just before you go there, what you just described, in the legal terminology, it is known as the consumer harm standard. And this has kind of defined US antitrust law for the past few decades, which is they judge everything on the standard of consumer harm. Is this going to harm consumers or benefit consumers? And if they find that it's gonna harm consumers,

Piyush (33:58)
Okay.

Right.

Nikhil Maddirala (34:15)
then they try to curtail that kind of behavior. But over the past five to 10 years or so, there's been a shift, there's been a new administration and this person, Lena Kahn, who heads the FTC has pursued a more aggressive antitrust policy where they're saying even if there is no consumer harm right now,

Piyush (34:18)
Right.

Nikhil Maddirala (34:39)
We still, in principle, want to prevent large companies from having excessive amounts of power because of potential future harm that they could cause, or just because we don't want entities to be so powerful that they're preventing competition, or they're amassing too much political power, and various other reasons.

Piyush (35:00)
Yeah, so actually that's the point I was going to make is I agree that there could be other types of harm, but whenever people think of monopoly and the harm that it causes, it's generally if you ask anyone on the street, they will say, yeah, it's to prevent consumer harm. But in this case, what's interesting to me is it might actually have the opposite effect because a lot of these companies, because of their economies of scale, are able to provide amazing services essentially for free. Like a lot of people would say, what is?

Nikhil Maddirala (35:27)
Yeah.

Piyush (35:29)
And I know there's argument to be made on both sides, but for most people, were like, Google's amazing. I get this amazing search engine for free. I get email for free. I get maps for free. Like these are amazing. People depend on these services daily. Like there's a very famous principle that Larry Page had in order to like shut down a potential business idea internally. And he called it the toothbrush principle. I don't know if you've heard of it. And his

Nikhil Maddirala (35:57)
Hmm.

Piyush (35:59)
framework was, whatever you're pitching, does it have the potential to be used at least twice a day, like a toothbrush would be? Because that's something that I'm interested in, not that he was saying that all businesses that don't meet the two intervals are not a good idea. Or Larry Page, right? Or Google Scale or Larry Page to be interested or excited about it should meet that. And a lot of these services, which are for free, more than meet the toothbrush criteria, like Google Search, Google Maps. So I'm curious, how do you think about that? Because when you...

Nikhil Maddirala (36:09)
But for Google scale, that's what they... Yeah.

Piyush (36:29)
try to intervene or regulate these things, it might end up having the opposite effect because when they lose the economies of scale, maybe the service quality would go down and no one was being charged for it anyways. So now consumers would potentially be charged for something which was either free or get a sub standard quality service.

Nikhil Maddirala (36:51)
I absolutely agree. think this is just a very complex issue. I don't have any like easy answer for this. But I think one thing that people are converging on now is that consumer harm is not the only standard that we can apply. There are other effects that we have to consider, like maybe just market dominance, their killing competition, and it's just maybe like dangerous for...

single company to have like so much power in a democratic society. So I don't know, it's like a very complex issue, but it's really evolving because I think this is one of the first times in history where these companies have like probably a lot more power than any governments right now. Like decisions that are made by companies like Apple, Meta, Google are I think a lot more impactful on the day -to -day lives of individuals.

than decisions that most government officials can make. Even the president of the US, I would argue, who's traditionally supposed to be one of the most powerful people in the world, I think, probably Zuckerberg and Tim Cook and Sundar Pichai have as much, if not more, influence on the lives of day -to -day people than he does. So yeah, it's pretty complex.

Piyush (38:09)
actually are even to potentially influence the election of said government. Criticism was made to Meta earlier because of the Cambridge Analytica scandal. A lot of people make the same criticism of YouTube or X, formerly known as Twitter, that these big giant platforms actually have the influence to sway people's opinion about a particular political candidate or not. And a lot of these algorithms can control what's seen, what's

subordered so you are right, like not just on the daily lives of people but potentially just having power over the formation of the government itself. So yeah, I can see that point. So you're saying that traditionally the standard that was applied was consumer harm but there's more to it. and this is.

Nikhil Maddirala (38:59)
So the benefit of the consumer harm standard is that it's actually easy to measure. And this was, I think, an innovation pioneered by the Chicago School of Economics, they were in the 60s or 70s, something like that, where they promoted this standard. And they were like, hey, this is actually a real objective way of measuring harm and is a standard we should use rather than people. Because otherwise, if you don't have a clear standard.

then it just comes down to people making subjective, biased decisions based on various other motivations. So that's one of the benefits of this consumer harm standard that is a clear objective criteria. But often this is the challenge that maybe the thing that you can measure.

is not the most important thing. So yeah, and it's hard to measure the thing that you actually care about, which is like, how do I measure when a company has too much power? so, yeah, I don't know.

Piyush (39:47)
Yeah.

Yeah.

So I am curious, what do you think are the outcomes here? Let's play that hypothetical thought experiment. So let's say this ruling.

Nikhil Maddirala (40:04)
So let's think about a narrow outcome and then like broader outcomes. Narrow outcomes is let's focus on this thing like the Apple Google arrangement. What

Piyush (40:12)
Yeah, so I want to hear your thoughts. What do you think happens? Let's say this ruling sticks, and now Google can make this deal with Apple. What do you think happens next? Does Apple provide an option every time someone searches the first time? It's like, hey, which service would you like to choose? Does Apple launch its own? I'm just riffing.

Nikhil Maddirala (40:17)
Yeah.

Hmm

Yeah, these are actually the range of outcomes. So one is that Apple can actually make a deal with another search provider. For example, they could make a deal with Bing for Bing to become the default, Microsoft's Bing. And that actually is not illegal because Microsoft...

Piyush (40:45)
because they don't have the market share.

Nikhil Maddirala (40:46)
Yeah, it does not have a monopoly. Yeah, it's illegal for a monopolist to make this type of exclusive deal, but for a non -monopolist, it's not. But actually, one thing that was leaked was that some Apple executives said that, hey, there's no amount of money Microsoft could pay us to make Bing the default, because I think the implication is that they think it sucks so much.

Piyush (41:01)
He

It's not as good, which is an interesting point, right? Which is what I was saying earlier. Now this might force Apple to do something which is detrimental to Apple users. So Apple was in such a good position because not only were Apple users getting the best search experience, arguably, right? I would say Google Search is the best search. I think most people would agree. But they were getting paid $20 billion to do so it's the best deal in the world. So now if they're forced to get rid of this deal, this is...

Nikhil Maddirala (41:22)
Yeah, for sure.

Yeah

Piyush (41:33)
a detrimental experience to Apple users. Yeah, it's interesting.

Nikhil Maddirala (41:37)
So either they make another deal, that's one option, seems unlikely, or they can build their own search, which again, I don't know if they have the appetite for that, or they just present a choice screen to all the users. And the choice screen also actually, I think raises interesting dynamics, which is, well, which option do you show first when you say here's a choice of

Piyush (41:46)
Sorry.

Nikhil Maddirala (42:02)
all the options you can use, whichever is the first one, probably like a lot of users will just pick that and can you pay to be like the first in the list instead of paying to be the default? I don't know, these details will have to get ironed out. But I think it's interesting to think about this in the context of AI because it seems that Apple was heading towards a similar strategy for AI and now maybe they will have to rethink that. I mean, I'm not sure.

Piyush (42:10)
Right. Right.

I have an interesting, yeah, I have a thought, like a prediction. Maybe what Apple can do here is become like a perplexity. So I don't know if this would fall under the ruling, but they could use Google search rankings API the way perplexity does and become a search engine where they don't have to invest in all the hard work of searching, like ranking and indexing, but just summarize based on the ranking that output that they get from the API. So basically Apple could become

a perplexity.

Nikhil Maddirala (42:59)
Yeah, actually I heard some people propose that Apple should just buy perplexity, that they could go acquire perplexity and that would have a similar...

Piyush (43:08)
That would be a good move for Apple actually, yeah. Just acquire perplexity. That would be a good experience as well. Integrate that with Apple intelligence for search. So basically, anytime someone asks. And the orchestrator could be.

Nikhil Maddirala (43:13)
Yeah.

So this is a version.

This is version of Apple building their own search then and building their own AI and combining AI with search. instead of build, they're like buying either one is fine. But yeah, I think that's an interesting strategy for Apple to pursue rather than I think that this will harm them in the long run if they keep outsourcing these like core capabilities, especially AI, which I think is going to become more and more important. I think they likely want to have

Piyush (43:26)
Yeah.

Yeah. Yeah.

Nikhil Maddirala (43:53)
of that capability in -house and not outsource it for this type of deal. But yeah, I don't know. And the other implication that's worth noting is what happens to Google. And there's been talk of maybe Google being broken up into multiple companies, which we touched upon at the start of the episode.

Piyush (44:11)
Yeah, that part I didn't understand. How is breaking up Google a solution to search monopoly? Well, it depends on how the breakup is being thought about. Do you have any insights into what does that even mean, breaking up Google? Is it into separate companies?

Nikhil Maddirala (44:26)
Yeah, I think the concern here is that they're leveraging their monopoly position in one market to get an unfair advantage in other markets. That's the main concern that they're a

Piyush (44:38)
You mean exam?

Nikhil Maddirala (44:40)
like they are a monopoly in search and they're using that to let's say gain dominance in like video as a market like YouTube for example because let you know they can say for advertisers is this really easy if you're already advertising on Google I'll integrate all the YouTube advertising to you and like then that's like unfair in the video space

Because these guys are already like so dominant in in one market and they're leveraging that to to gain dominance in the other markets. So

Piyush (45:17)
So this whole case that's going on treating Google as a monopoly, there's many different facets to it, is what you're saying. There's the Apple search deal facet. Then there's the one business advantageing another business within Google. OK. So the three.

Nikhil Maddirala (45:31)
Mm -hmm.

Yeah, these are all like the implications of what you cannot do once you're a monopolist. Once you're a monopolist, you cannot try to retain your monopoly status through making exclusive deals. You cannot leverage that monopoly status to go and try and gain dominance over a different market. So that's why these are some of the solutions being proposed. It's very early stages right now. I think we should just wait and watch and see what happens. But yeah, maybe thinking about AI.

Piyush (45:38)
Right.

All right.

Nikhil Maddirala (46:03)
things we've been talking about from the beginning about AI is whether AI is providing new opportunities for new companies to rise up and gain market share or whether it's just empowering existing incumbent companies to further entrench their dominance and so far it kind of looks like it's the the incumbents are the ones that are winning with AI and

That's been kind, I don't know, I mixed feelings about that, but I think this is further solidifying that case, that so far AI, it's been something that big companies are able to get more value from, and we haven't really seen that much AI value creation from smaller companies.

Piyush (46:54)
Do you think there is a possibility that one of these companies will make such a powerful AI model that it would be nationalized or something? Like, do you think that's in the realm of possibility?

Nikhil Maddirala (47:06)
I have no idea. mean, probably in China if that happens, yeah.

Piyush (47:09)
I mean, to the point you were making earlier, like Mark Zuckerberg, Sundar Pichai, Cook having more influence on people's day -to -day lives than even the President of United States, who's traditionally thought of as the most powerful person on earth, like this power will only get more influential with more and more powerful models. So, yeah, I wonder where that goes. Like, is it enough to break these companies up or?

Like should the government control a super powerful multimodal model that can smell and taste or what? I don't know.

Nikhil Maddirala (47:38)
Mm.

I don't know man, like don't really want like Donald Trump or Kamala Harris being like in charge of this like more so than like Zalk for example or like know Sundar Pichai but I mean of course this is very complex they are like democratically elected whichever one of them comes into power and their checks and balances on them but it's a really hard question I don't know

Piyush (48:11)
Yeah. Yeah. Wow. Interesting. there's, it's very, like, I say this in every episode, but I feel like, there is a, there is a theory that we're living in a simulation and there are so many different explanations that are given for this theory. One of them is that what are the chances that we are witnessing the most interesting time in human history ever. So I do believe that. And the reason I do believe that is like all of these developments that are happening right in front of our eyes.

Nikhil Maddirala (48:35)
Hmm.

Piyush (48:42)
I do think that these make for the most interesting time ever in human history. Or maybe that's just a bias that every human has had. Like maybe someone who was witnessing agriculture or everything was like, this is crazy. This is the most interesting thing ever. So who knows? But like, yeah, I don't know where all this

Nikhil Maddirala (48:52)
You

Well, but even the example you gave was like agricultural revolution. So if you look at these revolutions, there haven't been that many of those in human history, like agriculture, industry, computers. so we're definitely living through one of the more interesting revolutions and times of change. And I think the pace of change is just so much faster now than like, I think one example.

Piyush (49:07)
Yeah.

Nikhil Maddirala (49:24)
that's often given is go to any time in history and think about if a person from a hundred years ago was transported into your world today, like how new would it be? How surprised would they be? And I think today that surprise factor is way higher than at any other time in human history.

Piyush (49:42)
Yeah.

Yeah, for sure. it's going to be, to your point, the way it's accelerating, that surprise factor might be true for us within our generation. Like you use the 100 year time span. It could be that just 25 years down the line, if someone went into a coma right now and woke up 20 years down the line, I think their head would spin more than like a 100 year experiment, maybe.

Nikhil Maddirala (49:57)
Mm -hmm.

Yeah.

Yeah.

Yeah, well I hope we can record another podcast like this 25 years later and compare it to what it was.

Piyush (50:20)
No, that's what I like about our podcast. And maybe this is a good to end on is like we're documenting through our conversation, this very, very interesting thing that's evolving in front of our eyes. And it's like, no one knows what's going to happen. One thing I know for sure is someone's claiming like, this is exactly how this will shape up. They're full of BS. Like this is just so hard to predict. So it's just interesting because it's all bets are off. We, no one knows what's happening, but it's very, very interesting nonetheless.

Nikhil Maddirala (50:40)
Yeah.

Actually, one last thing I want to share on that note is I have like, can't reveal exactly.

but like I've heard many like AI leaders in this space like say very explicitly like guys we have no idea right now what the future is gonna look like and the comparison I think Peter Thiel is the one who made this comparison it's like we are in the early days of what the internet was in the 1990s like everyone knew that this is huge and is gonna have like a lot of implications but no one in the

the 1990s had any idea what 2020 would look like and what the internet would bring. So I think most people recognize that that's where we are right now with AI. Everyone knows it's huge, but no one knows exactly in what way it's gonna impact us. So yeah, we're not alone. think that there's no one out there who has these answers.

Piyush (51:47)
Yeah, but I am looking forward to documenting this hopefully every week with you as we are witnessing this very, interesting phenomenon unfold in front of our eyes.

Nikhil Maddirala (51:53)
Yeah. Yeah.

Yeah, well, this was a super interesting conversation. Thank you. Thank you to all of our listeners who are still here with us. If you're still listening, please make sure to leave us a rating and review in whatever app you're using right now to listen to this and send us your comments and feedback. Okay, see ya.

Piyush (52:20)
All right. Have a good day, Nico. Bye.



# Post-episode Notes
## Title
S2-E9: Google AI announcements and legal challenges

1. ChatGPT
	1. "Google's AI Gemini and Antitrust Battle: A Deep Dive"
	2. "AI Innovation at Google: Unpacking the Pixel Event and Legal Challenges"
	3. "From Gemini AI to Antitrust: Googles Future in Focus"
2. Riverside
	1. From Text to Multimodal: The Future of AI 
	2. Expanding Human Capabilities: The Path to AGI
	3. AI's Role in Shaping the Future of Tech
	4. The Power Dynamics of Big Tech Companies


## Description
In this episode Nikhil and Piyush discuss Google's recent AI announcements from the Google Pixel event and the recent antitrust ruling in the US declaring Google to be a monopolist. We discuss new AI capabilities in Google Pixel such as Gemini Live, multimodal AI, and the challenges of integrating LLMs into AI assistants. We then discuss the Google antitrust ruling and its implications for Google and Apple. The conversation also touches on the power dynamics of big tech companies, the future potential of AI to expand human sensory consciousness and the implications for AGI!

- Riverside
	- In this episode, Nikhil and Piyush discuss the recent AI announcements from Google, including the launch of the Pixel phones and the associated AI features. They focus on the new feature called Gemini Live, which allows real-time interaction with the Google AI model. They also discuss the challenges of integrating different modalities, such as text, image, audio, and video, into AI models. The conversation touches on the future potential of AI to expand human sensory consciousness and the implications for AGI. In this conversation, Nikhil and Piyush discuss the recent Google event and the ruling that declared Google a monopolist in an antitrust case. They cover features like the magic editor in Google Photos and the call notes feature. They also explore the implications of the ruling on the Apple-Google search deal and the potential breakup of Google. The conversation delves into the role of AI in these developments and the power dynamics of big tech companies. They conclude by reflecting on the rapid pace of change and the uncertainty of the future.
- ChatGPT
	- In this episode of _Art and Science of AI_, hosts Nikhil Maddirala and Piyush Agarwal take a closer look at Google's recent AI advancements and the legal battles the tech giant faces. Tune in to learn about Google's new AI model, Gemini, which promises real-time interactions across multiple devices, and how it stacks up against competitors like Apples Siri. The conversation also touches on the potential breakup of Google as a monopoly and its implications for AI and search technology. This episode is a must-listen for anyone curious about the future of AI and the evolving landscape of tech giants.

## Chapters
- 00:00 Google Pixel event: AI features & Gemini
- 07:25 Google Gemini vs. Apple Intelligence
- 27:55 Google antitrust ruling
- 46:54 Future of AI: tech giants vs. startups

Riverside
- 00:00 Google Pixel Event and AI Features
- 03:11 Gemini Live: Real-Time Interaction with Google AI
- 07:25 Google's Ecosystem and Integrated AI
- 13:00 Balancing Simple and Complex Tasks in AI Models
- 15:21 Gemini Live: Real-Time Interaction and New Context
- 19:19 Expanding Modalities in AI Models
- 25:06 AGI and Expanded Modalities
- 27:55 The Google Monopolist Ruling
- 37:48 The Apple-Google Search Deal
- 46:54 The Power Dynamics of Big Tech Companies
- 50:40 The Uncertain Future of AI
- 52:23 s2 outro.mp4

ChatGPT
- 00:00 - Introduction to the Episode
- 01:21 - Google Pixel Event: AI Features & Gemini
- 07:25 - Gemini AI vs. Apple Intelligence
- 27:26 - Google's Legal Challenges: Antitrust Case Overview
- 46:03 - AI: Empowering Giants or New Players?


## Takeaways
- Final
	- **AI in Google Pixel**: Google announced the launch of new Pixel phones with AI features centered around Gemini Live. However, there was no mention of any third-party app integrations that were announced in Apple Intelligence.
	- **Gemini Live:** Gemini Live allows real-time interaction with the Google AI model, combining voice input with access to the camera, screen, and app information. 
	- **Multimodal AI**: Integrating different modalities, such as text, image, audio, and video, into AI models is a complex challenge that researchers are working on. The ultimate goal is to develop multimodal models that can process and generate information in various formats, similar to human sensory consciousness.
	- **Google's monopoly status**: A US judge ruled that Google is a monopolist in an antitrust case, which has implications for Google, Apple, and raises questions about the power dynamics of big tech companies and the future of AI.
- ChatGPT
	- **Google's Gemini AI:** Google's new AI, Gemini, introduces real-time voice interactions and integrates with Google's ecosystem. However, it lacks third-party app integration, making it less competitive against Apples AI capabilities.
	- **AI Competition:** The episode discusses how Google and Apple are competing in the AI space, highlighting the potential for Apple to follow Googles model of integrating AI across its apps but with a different approach to third-party interactions.
	- **Antitrust Issues:** Google faces an antitrust case in the US, with the court ruling it as a monopoly in search advertising. This ruling could impact deals like Google's partnership with Apple for default search on Safari, which could change the landscape of AI integration in tech devices.
	- **The Future of AI:** There is an ongoing debate about whether AI will empower existing tech giants or create opportunities for new players in the market. So far, incumbents like Google seem to be strengthening their positions.
- Riverside
	- Google announced the launch of the Pixel phones and the associated AI features, with a focus on the new feature called Gemini Live.
	- Gemini Live allows real-time interaction with the Google AI model, combining voice input with access to the camera, screen, and app information.
	- Integrating different modalities, such as text, image, audio, and video, into AI models is a complex challenge that researchers are working on.
	- The ultimate goal is to develop multimodal models that can process and generate information in various formats, similar to human sensory consciousness.
	- The advancements in AI and the integration of different modalities have the potential to expand human capabilities and pave the way for AGI. Google introduced new features like the magic editor in Google Photos and the call notes feature for transcribing phone calls.
	- A US judge ruled that Google is a monopolist in an antitrust case, which has implications for the Apple-Google search deal.
	- The ruling raises questions about the power dynamics of big tech companies and the potential breakup of Google.
	- The conversation highlights the role of AI in these developments and the need to consider the impact on consumers and competition.


# Final show notes

## Substack version
S2-E9: Google AI announcements and legal challenges

In this episode Nikhil and Piyush discuss Google's recent AI announcements from the Google Pixel event and the recent antitrust ruling in the US declaring Google to be a monopolist. We discuss new AI capabilities in Google Pixel such as Gemini Live, multimodal AI, and the challenges of integrating LLMs into AI assistants. We then discuss the Google antitrust ruling and its implications for Google and Apple. The conversation also touches on the power dynamics of big tech companies, the future potential of AI to expand human sensory consciousness and the implications for AGI!

##  Listen to the episode
EMBED YOUTUBE

Art and Science of AI is available wherever you watch or listen to podcasts! Click these links for[YouTube](https://www.youtube.com/@ArtScience-AI),[Spotify](https://open.spotify.com/show/2Wkubbc0AqF5492i94uFj7),[Apple Podcasts](https://podcasts.apple.com/us/podcast/art-and-science-of-ai/id1748468279),[Substack](https://artscienceai.substack.com/), or copy the[RSS link](https://anchor.fm/s/f67d7b4c/podcast/rss)into your favorite podcast app. Don't forget to click subscribe / follow in the app to get notified for new episodes!

Subscribe to our newsletter to get weekly updates by email! 

##  Key concepts
- **AI in Google Pixel**: Google announced the launch of new Pixel phones with AI features centered around Gemini Live. However, there was no mention of any third-party app integrations that were announced in Apple Intelligence.
- **Gemini Live:** Gemini Live allows real-time interaction with the Google AI model, combining voice input with access to the camera, screen, and app information. 
- **Multimodal AI**: Integrating different modalities, such as text, image, audio, and video, into AI models is a complex challenge that researchers are working on. The ultimate goal is to develop multimodal models that can process and generate information in various formats, similar to human sensory consciousness.
- **Google's monopoly status**: A US judge ruled that Google is a monopolist in an antitrust case, which has implications for Google, Apple, and raises questions about the power dynamics of big tech companies and the future of AI.

##  Links
- [Homepage](https://artscienceai.substack.com/about)
- [YouTube](https://www.youtube.com/@ArtScience-AI)
- [Spotify](https://podcasters.spotify.com/pod/show/art-science-ai)
- [Linkedin](https://www.linkedin.com/company/art-science-ai)
- [Facebook](https://www.facebook.com/ArtScienceAI)
- [Instagram](http://instagram.com/artscienceai/)
- [Threads](https://www.threads.net/@artscienceai)
- [Twitter / X](https://x.com/ArtScienceAI)

###  Keywords
`#AI #ArtificialIntelligence #GenerativeAI #GenAI #LLM #MachineLearning #ML #tech #podcast`


### Youtube / Spotify / etc.
In this episode Nikhil and Piyush discuss Google's recent AI announcements from the Google Pixel event and the recent antitrust ruling in the US declaring Google to be a monopolist. We discuss new AI capabilities in Google Pixel such as Gemini Live, multimodal AI, and the challenges of integrating LLMs into AI assistants. We then discuss the Google antitrust ruling and its implications for Google and Apple. The conversation also touches on the power dynamics of big tech companies, the future potential of AI to expand human sensory consciousness and the implications for AGI!

===  CHAPTERS ===
- 00:00 Google Pixel event: AI features & Gemini
- 07:25 Google Gemini vs. Apple Intelligence
- 27:55 Google antitrust ruling
- 46:54 Future of AI: tech giants vs. startups

===  KEY CONCEPTS ===
- AI in Google Pixel: Google announced the launch of new Pixel phones with AI features centered around Gemini Live. However, there was no mention of any third-party app integrations that were announced in Apple Intelligence.
- Gemini Live: Gemini Live allows real-time interaction with the Google AI model, combining voice input with access to the camera, screen, and app information. 
- Multimodal AI: Integrating different modalities, such as text, image, audio, and video, into AI models is a complex challenge that researchers are working on. The ultimate goal is to develop multimodal models that can process and generate information in various formats, similar to human sensory consciousness.
- Google's monopoly status: A US judge ruled that Google is a monopolist in an antitrust case, which has implications for Google, Apple, and raises questions about the power dynamics of big tech companies and the future of AI.

===  LINKS ===
- Homepage: https://artscienceai.substack.com/about
- YouTube: https://www.youtube.com/@ArtScience-AI
- Spotify: https://podcasters.spotify.com/pod/show/art-science-ai
- Linkedin: https://www.linkedin.com/company/art-science-ai
- Facebook: https://www.facebook.com/ArtScienceAI
- Instagram: http://instagram.com/artscienceai/
- Threads: https://www.threads.net/@artscienceai
- Twitter / X: https://x.com/ArtScienceAI

===  KEYWORDS ===
`#AI #GenAI #LLM #ChatGPT #podcast`



# Social Media Posts 
## Publish post (Monday)
- [x] Linkedin: article + post
- [ ] Facebook + IG: MBS
- [ ] Threads
- [ ] Twitter

For each post
- [ ] Title
- [ ] Description
- [ ] Tags
- [ ] Square image
- [ ] Keywords
- [ ] Links (Twitter / Threads - in comments)

## Shorts 1 (Wednesday)
- [ ] Youtube shorts
- [ ] Linkedin
- [ ] Facebook + IG: MBS
- [ ] Threads + Twitter: Buffer

## Shorts 2 (Friday)

