---
source-title: RAG From Scratch: Part 14 (ColBERT)
source-url: https://youtube.com/watch?v=cN6S0Ehm7_8

source-mediatype: video
source-platform: YouTube

source-youtube-video-id: cN6S0Ehm7_8
source-youtube-channel-id: UCC-lyoTfSrcJzA1ab3APAgw
source-youtube-channel-name: LangChain
---

## Summary (AI)
TLDR: In this video, the speaker discusses an approach called CoBear for indexing in semantic similarity search. Instead of compressing entire documents into single vectors, CoBear breaks documents into tokens and computes similarity at a token level. The process involves creating embeddings for each token in a document and a question, comparing similarities, and calculating the sum of max similarities for all token pairs. The speaker demonstrates how to use the CoBear approach using the Rouille library and provides steps to create an index and perform retrieval.

Key Points:
- Overview of indexing in semantic similarity search, focusing on embedding models and document compression to vectors.
- Introduction to the CoBear approach, which breaks documents and questions into tokens for similarity computation at a token level.
- The importance of positional weighting and token representation in CoBear.
- Demonstration of using the Rouille library to apply CoBear for indexing and retrieval.
- Considerations for performance latency and production readiness when implementing CoBear.
- CoBear offers a different algorithm for computing document similarity that may be suitable for longer documents and provides strong performance results.
- Encouragement to experiment with CoBear and explore its potential for improving retrieval efficiency.

## Video description
Error retrieving description for video cN6S0Ehm7_8

## Video transcript
hi this is Lance from Lang chain this is the 14th part of our rag from scratch series we're going to I'm going to be talking about an approach called cold bear um so we've talked about a few different approaches for indexing and just as kind of a refresher indexing Falls uh kind of right down here in our flow we started initially with career translation taking a question translating it in some way to optimize retrieval we talked about routing it to par database we then talked about query construction so going from natural language to the DSL or domain specific language for E any of the databases that you want to work with those are you know metadata filters for Vector stores or Cipher for graph DB or SQL for relational DB so that's kind of the flow we talked about today we talked about some indexing approaches like multi-representation indexing we gave a small shout out to GRE cam in the series on chunking uh we talked about hierarchical indexing and I want to include one Advanced kind of embedding approach so we talked a lot about embeddings are obviously very Central to semantic similarity search um and retrieval so one of the interesting points that's been brought up is that embedding models of course take a document you can see here on the top and embed it basically compress it to a vector so it's kind of a compression process you're representing all the semantics of that document in a single Vector you're doing the same to your question you're doing a similarity search between the question embedding and the document embedding um in order to perform retrieval you're typically taking the you know K most similar um document of betting is given a question and that's really how you're doing it now a lot of people have said well hey the compressing a full document with all this Nuance to single Vector seems a little bit um overly restrictive right and this is a fair question to ask um there's been some interesting approaches to try to address that and one is this this this approach method called Co bear so the intuition is actually pretty straightforward there's a bunch of good articles I link down here this is my little cartoon to explain it which I think is hopefully kind of helpful but here's the main idea instead of just taking a document and compressing it down to a single Vector basically single uh what we might call embedding Vector we take the document we break it up into tokens so tokens are just like you know units of of content it depends on the token areas you use we talked about this earlier so you basically tokenize it and you produce basically an embedding or vector for every token and there's some kind of positional uh waiting that occurs when you do this process so you obviously you look to look at the implementation understand the details but the intuition is that you're producing some kind of representation for every token okay and you're doing the same thing for your questions you're taking your question you're breaking into a tokens and you have some representation or vector per token and then what you're doing is for every token in the question you're Computing the similarity across all the tokens in the document and you're finding the max you're taking the max you're storing that and you're doing that process for all the tokens in the question so again token two you compare it to every token in the in the document compute the Max and then the final score is in this case the sum of the max similarities uh between every question token and any document token so it's an interesting approach uh it reports very strong performance latency is definitely a question um so kind of production Readiness is something you should look into but it's a it's an approach that's worth mentioning here uh because it's pretty interesting um let's walk through the code so there's actually a really nice Library called rouille which makes it very easy to play with Co bear um she's pip install it here I've already done that and we can use one of their pre-train models to mediate this process so I'm basically following their documentation this is kind of what they recommended um so I'm running this now hopefully this runs somewhat quickly I'm not sure I I previously have loaded this model so hopefully it won't take too long and yeah you can see it's pretty quick uh I'm on a Mac M2 with 32 gigs um so just as like a context in terms of my system um this is from their documentation we're just grabbing a Wikipedia page this is getting a full document on Miyazaki so that's cool we're going to grab that now this is just from their docs this is basically how we create an index so we provide the you know some index name the collection um the max document length and yeah you should look at their documentation with these flags these are just the default so I'm going to create my index um so get some logging here so it it's working under the hood um and by the way I actually have their documentation open so you can kind of follow along um so um let's see yeah right about here so you can kind of follow this indexing process to create an index you you to load a train a a trained model this can be either your own pre-train model or one of our from The Hub um and this is kind of the process we're doing right now create index is just a few lines of code and this is exactly what we're doing um so this is the you know my documents and this is the indexing step that we just we just kind of walked through and it looks like it's done um so you get a bunch of logging here that's fine um now let's actually see if this works so we're going to run rag search what an emotion Studio did Miaki found set our K parameter and we get some results okay so it's running and cool we get some documents out so you know it seems to work now what's nice is you can run this within L chain as the L chain retriever so that basically wraps this as a l chain Retriever and then you can use it freely as a retriever within Lang chain it works with all the other different LMS and all the other components like rankers and so forth that we talk through so you can use this directly as a retriever let's try this out and boom nice and fast um and we get our documents again this is a super simple test example you should run this maybe on more complex cases but it's pretty pretty easy spin up it's a really interesting alternative indexing approach um using again like we talked through um a very different algorithm for computing doc similarity that may work better I think an interesting regime to consider this would be longer documents so if you want like longer um yeah if if you basically want kind of long context embedding I think you should look into for example the uh Max token limits for this approach because it partitions the document into into each token um I would be curious to dig into kind of what the overall context limits are for this roach of Co bear but it's really interesting to consider and it reports very strong performance so again I encourage you to play with it and this is just kind of an intro to how to get set up and to start experimenting with it really quickly thanks
