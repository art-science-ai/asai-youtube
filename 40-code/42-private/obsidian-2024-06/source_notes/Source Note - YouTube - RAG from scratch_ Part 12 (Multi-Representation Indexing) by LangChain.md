---
source-title: RAG from scratch: Part 12 (Multi-Representation Indexing)
source-url: https://youtube.com/watch?v=gTCU9I6QqCE

source-mediatype: video
source-platform: YouTube

source-youtube-video-id: gTCU9I6QqCE
source-youtube-channel-id: UCC-lyoTfSrcJzA1ab3APAgw
source-youtube-channel-name: LangChain
---

## Summary (AI)
TLDR: The video discusses multi-representation indexing as a method for optimizing retrieval in Vector Stores by creating summaries of documents that are better suited for search and leveraging long context LMs for document generation.

Key Points:
- Indexing techniques for Vector Stores are crucial for optimizing retrieval.
- Multi-representation indexing is highlighted as a method to distill document content into optimized summaries for retrieval.
- The approach involves storing raw documents separately and creating summaries optimized for search.
- Implementation involves defining a vector store for summaries and a document store for full documents.
- By using a unique ID to reference summaries and full documents, efficient retrieval is achieved.
- Code examples demonstrate the process of creating summaries, indexing them, and retrieving full documents based on search queries.
- The method allows for easy retrieval of full documents by leveraging the summarized content for efficient searching.
- Utilizing this technique is beneficial, especially when working with long context LMs for document generation and retrieval optimization.

## Video description
Error retrieving description for video gTCU9I6QqCE

## Video transcript
hi this is Lance from Lang chain I'm going to talk about indexing uh and multi representation indexing in particular for the 12th part of our rag from scratch series here so we previously talked about a few different major areas we talk about query translation which takes a question and translates it in some way to optimize for retrieval we talk about routing which is the process of taking a question routing it to the right data source be it a vector store graph DB uh SQL B we talked about query construction we dug into uh basically quer construction for Vector stores but of course there's also text SQL text decipher um so now we're going to talk about indexing a bit in particular we're going to talk about index indexing techniques for Vector Stores um and I want to highlight one particular method today called multi-representation indexing so the high LEL idea here is derived a bit from a paper called proposition indexing when which kind of makes a simple observation you can think about decoupling raw documents and the unit to use for retrieval so in the typical case you take a document you split it up in some way to index it and then you embed the split directly um this paper talks about actually taking a document splitting it in some way but then using an llm to produce what they call call a proposition which you can think of is like kind of a distillation of that split so it's kind of like using an llm to modify that split in some way to distill it or make it like a crisper uh like summary so to speak that's better optimized for retrieval so that's kind of one highlight one piece of intuition so we've actually taken that idea and we've kind of built on it a bit in kind of a really nice way that I think is very well suited actually for long context llms so the idea is pretty simple you take a document and you you actually distill it or create a proposition like they showed in the prior paper I kind of typically think of this as just produce a summary of the document and you embed that summary so that summary is meant to be optimized for retrieval so might contain a bunch of keywords from the document or like the big Ideas such that when you embed the sumary you embed a question you do search you basically can find that document based upon this highly optimized summary for retrieval so that's kind of represented here in your vector store but here's the catch you independently store the raw document in a doc store and when you when you basically retrieve the summary in the vector store you return the full document for the llm to perform generation and this is a nice trick because at generation time now with long condex LM for example the LM can handle that entire document you don't need to worry about splitting it or anything you just simply use the summary to Pro like to create a really nice representation for fishing out that full dock use that full Dock and generation there might be a lot of reasons you want to do that you want to make sure the LM has the full context to actually answer the question so that's the big idea it's a nice trick and let's walk through some code here so we have a notebook all set up uh just like before we've doing some pip installs um set to maybe I Keys here for Lang Smith um kind of here's a diagram now let me show an example let's just load two different uh blog posts uh one is about agents one is about uh you know human data quality um and what we're going to do is let's create a summary of each of those so this is kind of the first step of that process where we're going from like the raw documents to summaries let's just have a look and make sure those Ran So Okay cool so the first DOC discusses you know building autonomous agents the second doc contains the importance of high quality human data and training okay so that's pretty nice we have our summaries now we're going to go through a process that's pretty simple first we Define a vector store that's going to index those summaries then we're going to Define what we call like our our document storage is going to store the full documents okay so this multi Vector retriever kind of just pulls those two things together we basically add our doc store we add this bite store is basically the the the full document store uh the vector store is our Vector store um and now this ID is what we're going to use to reference between the chunks or the summaries and the full documents that's really it so now for every document we'll Define a new Doc ID um and then we're basically going to like take our summary documents um and we're going to extract um for each of our summaries we're going to get the associated doc ID so there we go um so let's go ahead and do that so we have our summary docs which we add to the vector store we have our full doc doents uh our doc IDs and the full raw documents which are added to our doc store and then let's just do a query Vector store like a similarity search on our Vector store so memory and agents and we can see okay so we can extract you know from the summaries we can get for example the summary that pertains to um agents so that's a good thing now let's go ahead and run a query get relevant documents on our retriever which basically combines the summaries which we use for retrieval then the doc store which we use to get the full doc back so we're going to apply our query we're going to basically run this and here's the key Point we've gotten back the entire article um and we can actually if you want to look at the whole thing we can just go ahead and do this here we go so this is the entire article that we get back from that search so it's a pretty nice trick again we query with just memory agents um and we can kind of go back to our diagram here we created for memory and agents it searched our summaries it found the summary related to memory and agents it uses that doc ID to reference between the vector store and the doc store it fishes out the right full doc returns us the full document in this case the full web page that's really it simple idea nice way to go from basically like nice simple proposition style or summary style indexing to full document retrieval which is very useful especially with long contact LMS thank you
