---
source-title: RAG from scratch: Part 9 (Query Translation -- HyDE)
source-url: https://youtube.com/watch?v=SaDzIVkYqyY

source-mediatype: video
source-platform: YouTube

source-youtube-video-id: SaDzIVkYqyY
source-youtube-channel-id: UCC-lyoTfSrcJzA1ab3APAgw
source-youtube-channel-name: LangChain
---

## Summary (AI)
TLDR: The video discusses the technique of queer translation, specifically focusing on the approach called HIDE. HIDE involves mapping questions into document space by generating hypothetical documents to improve retrieval in high-dimensional embedding space. The process includes generating a hypothetical document, retrieving related documents, and using the retrieved documents to answer the initial question through the RAG prompt.

Key Points:
- Queer translation in the RAG flow involves translating input questions to improve retrieval.
- HIDE is a technique that maps questions into document space using hypothetical documents.
- Documents and questions are different text objects, with HIDE aiming to make questions better suited for retrieval.
- The video includes a code walkthrough to demonstrate the implementation of HIDE, which involves generating a hypothetical document, retrieving related documents, and using the retrieved documents to answer the question.
- HIDE can be useful in certain domains for improving retrieval performance by generating hypothetical documents and tuning them for specific interests.

## Video description
Error retrieving description for video SaDzIVkYqyY

## Video transcript
hi this is Lance from Lang chain this is the fifth video focused on queer translation in our rag from scratch series we're really be talking about a technique called hide so again queer translation sits kind of at the front of the overall rag flow um and the objective is to take an input question and translate it in some way that improves retrieval now hide is an interesting approach that takes advantage of a very simple idea the basic rag flow takes a question and embeds it takes a document and embeds it and looks for similarity between an embedded document and embedded question but questions and documents are very different text objects so documents can be like very large chunks taken from dense um Publications or other sources whereas questions are short kind of tur potentially ill worded from users and the intuition behind hide is take questions and map them into document space using a hypothetical document or by generating a hypothetical document um that's the basic intuition and the idea kind of shown here visually is that in principle for certain cases a hypothetical document is closer to a desired document you actually want to retrieve in this you know High dimensional embedding space than the sparse raw input question itself so again it's just kind of means of trans translating raw questions into these hypothetical documents that are then better suited for retrieval so let's actually do a Code walkthrough to see how this works and it's actually pretty easy to implement which is really nice so first we're just starting with a prompt and we're using the same notebook that we've used for prior videos we have a blog post on agents already indexed um so what we're going to do is Define a prompt to generate a hypothetical documents in this case we'll say write a write a paper pass Mage uh to answer a given question so let's just run this and see what happens again we're taking our prompt piping it to to open AI CH gbt and then using string Opa parer and so here's a hypothetical document section related to our question okay and this is derived of course from lm's kind of embedded uh kind of World Knowledge which is you know a sane place to generate hypothetical documents now let's now take that hypothetical document and basically we're going to pipe that into a retriever so this means we're going to fetch documents from our index related to this hypothetical document that's been embedded and you can see we get a few qu a few retrieved uh chunks that are related to uh this hypothetical document that's all we've done um and then let's take the final step where we take those retrieve documents here which we defined and our question we're going to pipe that into this rag prompt and then we're going to run our kind of rag chain right here which you've seen before and we get our answer so that's really it we can go to lsmith and we can actually look at what happened um so here for example this was our final um rag prompt answer the following question based on this context and here is the retrieve documents that we passed in so that Parts kind of straightforward we can also look at um okay this is our retrieval okay now this is this is actually what we we generated a hypothetical document here um okay so this is our hypothetical document so we've run chat open AI we generated this passage was our hypothetical document and then we've run retrieval here so this is basically showing hypothetical document generation followed by retrieval um so again here was our passage we passed in and then here's our retrieve documents from the retriever which are related to the passage content so again in this particular index case it's possible that the input question was sufficient to retrieve these documents in fact given prior examples I know that some of these same documents are indeed retrieved just from the raw question but in other context that may not be the case so folks have reported nice performance using Hyde uh for certain domains and the Really convenient thing is that you can take this this document generation prompt you can tune this arbitrarily for your domain of Interest so it's absolutely worth experimenting with it's a it's a need approach uh that can overcome some of the challenges with retrieval uh thanks very much
