---
source-title: RAG from scratch: Part 11 (Query Structuring)
source-url: https://youtube.com/watch?v=kl6NwWYxvbM

source-mediatype: video
source-platform: YouTube

source-youtube-video-id: kl6NwWYxvbM
source-youtube-channel-id: UCC-lyoTfSrcJzA1ab3APAgw
source-youtube-channel-name: LangChain
---

## Summary (AI)
**TLDR:** 
Lance from Lang Chain discusses the process of query construction, specifically focusing on converting natural language into domain-specific language for vector stores. By using function calling and structured output, they demonstrate how to convert natural language questions into structured query objects for metadata filtering on vector stores.

**Key points:**
- Query construction involves converting natural language into specific domain language for vector stores.
- The process aims to create structured queries for metadata filters on vector stores.
- By using function calling and structured output, natural language questions can be converted into structured query objects.
- The example involves setting up a search object encapsulating information about available filters for range filtering and semantic searches.
- A tutorial search object is created to define available searches that can be done over different metadata fields.
- Function calling is used to bind a semantic object containing index information to a model, enabling structured output versus JSON string from natural language questions.
- The flow involves converting the search object to a function schema for the model to produce structured objects out of natural language inputs.
- A query analyzer chain is set up to run semantic and structured inputs resulting in content search, title search, and date search outcomes.
- The strategy can be broadly applied to various querying tasks, aiding in metadata filtering on vector stores from natural language questions.

## Video description
Error retrieving description for video kl6NwWYxvbM

## Video transcript
hi this is Lance from Lang chain this is the 11th part of our rag from scratch video series focused on query construction so we've previously talked through uh query translation which is the process of taking a question and converting it or translating it into a question that's better optimized for retrieval then we talked about routing which is the process of going taking that question routing it to the right Source be it a given Vector store graph DB um or SQL DB for example now we're going to talk about the process of query construction which is basically taking natural language and converting it into particular domain specific language uh for one of these sources now we're going to talk specifically about the process of going from natural language to uh meditated filters for Vector Stores um the problem statement is basically this let's imagine we had an index of Lang Chain video transcripts um you might want to ask a question give me you know or find find me videos on chat Lang ch published after 2024 for example um the the process of query structuring basically converts this natural language question into a structured query that can be applied to the metadata uh filters on your vector store so most Vector sour will have some kind of meditative filters that can do kind of structure querying on top of uh the chunks that are indexed um so for example this type of query will retrieve all chunks uh that talk about the topic of chat Lang chain uh published after the date 2024 that's kind of the problem statement and to do this we're going to use function calling um in this case you can use for example open AI or other providers to do that and what we're going to do is at a high level take the metadata fields that are present in our Vector store and provide them to the model as kind of information and the model then can take those and produce queries that adhere to the schema provided um and then we can pars those out to a structured object like a identic object which again which can then be used in search so that's kind of the problem statement and let's actually walk through code um so here's our notebook which we've kind of gone through previously and I'll just show you as an example let's take a example YouTube video and let's look at the metadata that you get with the transcript so you can see you get stuff like description uh URL um yeah publish date length things like that now let's say we had an index that had um basically a had a number of different metadata fields and filters uh that allowed us to do range filtering on like view count publication date the video length um or unstructured search on contents and title so those are kind of like the imagine we had an index that had uh those kind of filters available to us what we can do is capture that information about the available filters in an object so we're calling that a this tutorial search object kind of encapsulates that information about the available searches that we can do and so we basically enumerate it here content search and title search are semantic searches that can be done over those fields um and then these filters then are various types of structure searches we can do on like the length um The View count and so forth and so we can just kind of build that object now we can set this up really easily with a basic simple prompt that says you know you're expert can bring natural language into database queries you have access to the database tutorial videos um given a question return a database query optimize retrieval so that's kind of it now here's the key point though when you call this llm with structured output you're binding this pantic object which contains all the information about our index to the llm which is exactly what we talked about previously it's really this process right here you're taking this object you're converting it to a function schema for example open AI you're binding that to your model and then you're going to be able to get um structured object out versus Json string from a natural language question which can then be parsed into a pantic object which you get out so that's really the flow it's taking advantage of function calling as we said so if we go back down we set up our query analyzer chain right here now let's try to run that just on a on a purely semantic input so rag from scratch let's run that and you can see this just does like a Content search sech and a title search that's exactly what you would expect now if we pass a question that includes like a date filter let's just see if that would work and there we go so you kind of still get that semantic search um but you also get um search over for example publish date earliest and latest publish date kind of as as you would expect let's try another one here so videos focus on the topic of chat Lang chain they're published before 2024 this is just kind of a rewrite of this question in slightly different way using a different date filter and then you can see we can get we get content search title search and then we can get kind of a date search so this is a very general strategy that can be applied kind of broadly to um different kinds of querying you want to do it's really the process of going from an unstructured input to a structured query object out following an arbitrary schema that you provide and so as noted really this whole thing we created here this tutorial search is based upon the specifics of our Vector store of interest and if you want to learn more about this I link to some documentation here that talks a lot about different uh types of of Integrations we have with different Vector store providers to do exactly this so it's a very useful trick um it allows you to do kind of query uh uh say metadata filter filtering on the fly from a natural language question it's a very convenient trick uh that works with many different Vector DBS so encourage you to play with it thanks
