---
source-title: RAG From Scratch: Part 13 (RAPTOR)
source-url: https://youtube.com/watch?v=z_6EeA2LDSw

source-mediatype: video
source-platform: YouTube

source-youtube-video-id: z_6EeA2LDSw
source-youtube-channel-id: UCC-lyoTfSrcJzA1ab3APAgw
source-youtube-channel-name: LangChain
---

## Summary (AI)
TLDR: The video discusses a technique called Raptor for hierarchical indexing within Vector Stores. Raptor is used to answer both low-level and high-level questions by summarizing clusters of documents recursively. By building a hierarchical index of document summaries, Raptor allows for semantic search across different levels of information abstraction.

Key points:
- Raptor is a technique for hierarchical indexing within Vector Stores, allowing for better semantic coverage across different levels of question types.
- The technique involves clustering similar documents, summarizing each cluster, and recursively building a hierarchical index of document summaries.
- By collapsing these summaries together, Raptor creates a coherent array of chunks spanning the abstraction hierarchy, catering to both detailed and consolidated information needs.
- The video demonstrates applying Raptor to Lang chain documents, involving text embedding, clustering, summarization, and tree building processes.
- Raptor is particularly useful for handling large documents without the need for splitting, providing a comprehensive approach to information retrieval across multiple documents.
- The technique is detailed in the video, with further information available in a deep dive video linked in the description.

## Video description
Error retrieving description for video z_6EeA2LDSw

## Video transcript
hi this is Lance from Lang chain this is the 13th part of our rag from scratch series focused on a technique called Raptor so Raptor sits within kind of an array of different indexing techniques that can be applied on Vector Stores um we just talked about multi-representation indexing um we I prived a link to a video that's very good talking about the different means of chunking so I encourage you to look at that and we're going to talk today about a technique called Raptor which you can kind of think about as a technique for hierarchical indexing so the high LEL intuition is this some questions require very detailed information from a corpus to answer like pertain to a single document or single chunk so like we can call those lowlevel questions some questions require consolidation across kind of broad swast of a document so across like many documents or many chunks within a document and all those like higher level questions and so there's kind of this challenge in retrieval and that typically we do like K nearest neighbor retrieval like we've been talking about you're fishing out some number of chunks but what if you have a question that requires information across like five six you know or a number of different chunks which may exceed you know the K parameter in your retrieval so again when you typically do retrieval you might set a k parameter of three which means you're retrieving three chunk junks from your vector store um and maybe you have a high very high level question that could benefit from infation across more than three so this technique called raptor is basically a way to build a hierarchical index U of document summaries and the intuition is this you start with a set of documents as your Leafs here on the left you cluster them and then you Summarize each cluster so each cluster of similar documents um will consult information from across your context which is you know your context could be a bunch of different splits or could even be across a bunch of different documents you're basically capturing similar ones and you're consolidating the information across them in a summary and here's the interesting thing you do that recursively until either you hit like a limit or you end up with one single cluster that's like kind of very high level summary of all of your documents and what the paper shows is that if you basically just collapse all these and index them together as a big pool you end up with a really nice array of chunks that span the abstraction hierarchy like you have a bunch of chunks from Individual documents that are just like more detailed chunks pertaining to that you know single document but you also have chunks from the summaries or I would say like you know maybe not chunks but in this case the summary is like a distillation so you know raw chunks on the left that represent your leaves are kind of like the rawest form of information either raw chunks or raw documents and then you have these higher level summaries which are all indexed together so if you have higher level questions they should basically be more similar uh in semantic search for example to these higher level summary chunks if you have lower level questions then they'll retrieve these more lower level chunks and so you have better semantic coverage across like the abstraction hierarchy of question types that's the intuition they do a bunch of nice studies to show that this works pretty well um I actually did a deep dive video just on this which I link below low um I did want to cover it briefly just at a very high level um so let's actually just do kind of a code walk through and I've added it to this rack from scratch course notebook but I link over to my deep dive video as well as the paper and the the full code notebook which is already checked in and is discussed at more length in the Deep dive the technique is a little bit detailed so I only want to give you very high levels kind of overview here and you can look at the Deep dive video if you want to go more depth again we talked through this abstraction hierarchy um I appli this to a large set of Lang chain documents um so this is me loading basically all of our Lang chain expression language docs so this is on the order of 30 documents you can see I do a histogram here of the token counts per document some are pretty big most are fairly small less than you know 4,000 tokens um and what I did is I indexed all of them um individually so so all those raw documents you can kind of Imagine are here on the left and then I do um I do embedding I do clustering summarization and I do that recursively um until I end up with in this case I believe I only set like three levels of recursion and then I save them all my Vector store so that's like the high level idea I'm applying this Raptor technique to a whole bunch of Lang chain documents um that have fairly large number of tokens um so I do that um and yeah I use actually use both Claude as well as open AI here um this talks through the clustering method which they that they use which is pretty interesting you can kind of dig into that on your own if if you're really um interested this is a lot of their code um which I cite accordingly um this is basically implemented the clustering method that they use um and this is just simply the document embedding state page um this is like basically embedding uh and clustering that's really it some text formatting um summarizing of the clusters right here um and then this is just running that whole process recursively that's really it um this is tree building so basically I have the rod the rod docks let's just go back and look at Doc texts so this should be all my raw documents uh so that's right you can see it here doc text is basically just the text and all those Lang chain documents that I pulled um and so I run this process on them right here uh so this is that recursive embedding cluster basically runs and produces that tree here's the results um this is me just going through the results and basically adding the result text to this list of uh texts um oh okay so here's what I do this Leaf text is all the raw documents and I'm appending to that all the summaries that's all it's going on and then I'm indexing them all together that's the key Point rag chain and there you have it that's really all you do um so anyway I encourage you to look at this in depth it's a pretty interesting technique it works well long with long contexts so for example one of the arguments I made is that it's kind of a nice approach to consulted information across like a span of large documents like in this particular case my individual ments were L expression language docs uh each each being somewhere in the order of you know in this case like you know most of them are less than 4,000 tokens some pretty big but I index them all I cluster them without any splits uh embed them cluster them build this tree um and go from there and it all works because we now have llms that can go out to you know 100 or 200,000 up to a million tokens and context so you can actually just do this process for big swats of documents in place without any without need splitting uh it's a pretty nice approach so I encourage you to think about it look at it watch the deep that video If you really want to go deeper on this um thanks
